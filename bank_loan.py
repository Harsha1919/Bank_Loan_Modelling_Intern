# -*- coding: utf-8 -*-
"""Bank_Loan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PdnXINhIohOVecOfjGLgKwui-dwdA2ut

# ***------------ This is my Internship Project assigned to me by Internship Studio -------------***

### **Firstly we import the necessary libraries :**
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing all the necessary libraries required for the analysing of this dataset

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

"""### **now we load the Dataset :**"""

from google.colab import files
uploaded = files.upload()

# Loading the dataset

import io
data = pd.read_csv(io.BytesIO(uploaded['Bank_Loan.csv']))

"""### **we can see the dataset below :**"""

# The dataset can be seen here
data

"""### **Now we check the datatype of the dataset :**"""

data.head(10)

data.tail(10)

"""#### Now here we see all the datypes involved in the dataset for every column"""

data.dtypes

"""### **Now we get the summary and shape of the given dataset :**"""

# "describe()" this function describes the data in all aspects like minimum, maximum and all...

data.describe()

# We will know that how many rows and columns were there in the dataset in this particular cell by using "count"

rows_count, columns_count = data.shape
print('Total Number of rows :', rows_count)
print('Total Number of columns :', columns_count)

data.info()

"""## **Now we are going to check whether the dataset contains missing values or not :**"""

# "isnull()" this keyword is used to tell us whether the dataset contains the null values or not

data.isnull()

"""### we can see there is no missing value in the dataframe, i.e False refers that the dataset does not contain the null values. You can see clearly in below section"""

data.isnull().sum()

data.isnull().values.any()

data.isnull().apply(pd.value_counts)

"""### from here you can come to know clearly is that the dataset doesnot contain any null values . The observation is *"NO MISSING VALUES"*

### **Now here we are Filtering the data**
"""

# Here the data is modified/filtered, this gives you an idea on "How to modify data?"" in a very particular method  

f_data = data.loc[(data['Age'] > 55) & (data['Income'] < 20) & (data['Family'] > 3)]

"""### The filtered data is displayed here i.e people with Age more than 55 and having income lessthan 20 and 4 per Family :"""

# The modified data can be seen clearly here.
# This is not necessary to do here but in some cases it will be helpful

f_data

"""### **Now we check whether the dataset contains Duplicate values or not**"""

# Here the duplicated data is assigned to the variable "dup" and it shows whether the data contains duplicated data or not.

dup = data.duplicated()

dup

"""### *By this we came to know that the dataset does not contains any Duplicate values*

### **We have find whether the dataset contains the invalid data(i.e negative values)**

#### *We use a function called **any()**. This helps to find whether it contains  invalid values or not*
"""

any(data<0)

"""#### *Now we know that the dataset contains invalid data.To know exactly where the invalid value is we use **describe()**, because this describes the whole dataset*"""

import io
data = pd.read_csv(io.BytesIO(uploaded['Bank_Loan.csv']))

data.describe()

"""#### we got the invalid value. It lies in **Experience Column** . So now we go for further Analysis"""

any(data['Experience'] < 0)

"""#### We can verify the presance of invalid value by code and it is verified above"""

data[data['Experience'] < 0]['Experience'].count()

nexp = data.loc[data['Experience'] < 0]

# Here we can see the dataset nexp contains only negative values in Experience column

nexp

"""#### This is the total data which we have negative values in the dataset provided"""

exp = data.loc[data['Experience'] > 0] 
nexp = data.Experience < 0
mylist = data.loc[nexp]['ID'].tolist()

# This is the list of rows which are having negative values in Experience column

mylist

nexp.value_counts()

"""#### for replacing invalid values we apply a code .*We are replacing the invalid values with valid values*"""

# Here we are replacing the all  negative values with the median of the Experience column in the dataset

med = data.loc[:,"Experience"].median()
data.loc[:, 'Experience'].replace([-1, -2, -3], [med, med, med], inplace=True)

any(data['Experience'] < 0)

nexp

"""#### now you can see that the dataset doesnot contain any invalid values now"""

nexp = data.loc[data['Experience'] < 0]

# Here we can see that the data is modified. Hence we can't find the negative values in the Experiance column.

nexp

"""### now we apply again **describe()** function. So that you can see the changed dataset"""

# modified data can be seen clearly here
data.describe()

"""### **Observation : Invalid values are turned to valid values**"""

experience = data['Experience']
age = data['Age']
corr = experience.corr(age)
corr

"""#### **Now we drop some unnecessary columns from the dataset**"""

# We drop these because these columns doesnot effect our requirement and we dont need this data. So we drop this as it is of no use 

data = data.drop(['ID','Experience','ZIP Code'], axis=1 )

data.head()

"""### **---------- EDA :**

#### *Number of unique in each column :*
"""

# Here we can get number of unique in each column

data.nunique()

"""#### *Univariate Analysis :*"""

sns.barplot(data['CD Account'])
plt.show()

plt.subplots()
plt.pie(data["Income"])
plt.show()

plt.hist(data.Income, edgecolor = 'blue')
plt.xlabel('Income')

sns.boxplot(data['Income'])

sns.boxplot(data['CCAvg'])
plt.show()

# Positively Skewed data

sns.distplot(data['Income'])

# Positively Skewed data

sns.distplot(data['Mortgage'])

"""#### As we came to know that the Income , Mortgage , CCAvg are the skewed data

#### *Bivariate Analysis :*
"""

sns.countplot(x='Family',hue='Income', data=data)
plt.show()

plt.subplot(5,3,1)
plt.hist(data.Age, color='lightblue', edgecolor = 'black')
plt.xlabel('Age')

plt.subplot(5,3,2)
plt.hist(data.Income, color='darkblue', edgecolor = 'black')
plt.xlabel('Experience')

plt.show()

# Violin plot is used to represant Personal Loan and Age

sns.violinplot(x='Personal Loan', y='Age', data=data, size=8)
plt.show()

sns.violinplot('Age','Income',  data=data,width=0.5, size=10)

sns.distplot(data['CCAvg'])

"""#### *Multivariate Analysis :*"""

sns.boxplot(x='Education', y='Income', hue='Personal Loan', data = data)
plt.show()

# Pairplot

sns.pairplot(data.iloc[:,1:])
plt.show()

## Pairplot
sns.pairplot(data,hue='Personal Loan',height=2.5)

"""#### We can find the people with **Zero Mortgage** in two ways :"""

mort = data.loc[data['Mortgage'] == 0]

mort.count()

"""#### Or We can also find the people with **Zero Mortgage** in this way :"""

data[data['Mortgage']==0]['Mortgage'].count()

"""#### We can find the people with **Zero creditcard** in two ways:"""

data["CreditCard"].value_counts().to_frame()

cred = data.loc[data['CreditCard'] == 0]

cred.count()

"""#### Or We can also find the people with **Zero creditcard** in this way:"""

data[data['CreditCard']==0]['CreditCard'].count()

"""### *Here we are finding the Value_counts for all categorical columns*"""

data['Family'].value_counts()

data['Education'].value_counts()

data['Securities Account'].value_counts()

data['CD Account'].value_counts()

data['Online'].value_counts()

data['Age'].value_counts().plot.bar(figsize = (15,5))
plt.title("credit card")
plt.ylabel('No.of credit cards')
plt.xlabel('Age')
plt.show()

sns.distplot(data[data["Personal Loan"] == 0]['Income'], color = 'b')
sns.distplot(data[data["Personal Loan"] == 1]['Income'], color = 'g')

sns.countplot(data.Family)
plt.show()

"""#### for transformation we apply Z-Score analysis as well as IQR"""

# For finding Z-Scores we import this library

from scipy.stats import zscore

# IQR

Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3-Q1
print(IQR)

data.corr()

# Heatmap

plt.figure(figsize=(20,5))
sns.heatmap(data.corr(), annot = True)

"""#### The observation from above heat map is Income and CCAvg is moderately correlated and Age and Experience is highly correlated"""

# Mean of the whole dataset 

data.mean()

# Median of the whole dataset

data.median()

"""### Z-Score analysis :"""

# Discovering Outliers with mathematical function , Here we use Z-Score analysis

from scipy import stats
z = np.abs(stats.zscore(data))
z

filter=(z< 3).all(axis=1)
filter

threshold = 3
print(np.where(z<3))

z_data = data[filter]
z_data



"""#### to reduce the skewness of the data we apply Transformations to the data."""

# We apply "log transform" to reduce the skewness of the data 

ldata= data[['Income','CCAvg']]
ldata=np.log(ldata+1)
ldata

sns.distplot(ldata['Income'])
plt.show()

sns.distplot(ldata['CCAvg'])
plt.show()

# Here again we apply another method to reduce the skewness of the i.e "Power Transform"
# we use "Yea-johnson" because it handles good for negative values aslo

from sklearn.preprocessing import PowerTransformer
p_t = PowerTransformer(method="yeo-johnson", standardize=False)
p_t.fit(data['Income'].values.reshape(-1,1))
income = p_t.transform(data['Income'].values.reshape(-1,1))
sns.distplot(income)
plt.show()

# Here again we apply another method to reduce the skewness of the i.e "Power Transform"
# we use "Yea-johnson" because it handles good for negative values aslo

from sklearn.preprocessing import PowerTransformer
p_t = PowerTransformer(method="yeo-johnson", standardize=False)
p_t.fit(data['CCAvg'].values.reshape(-1,1))
ccavg = p_t.transform(data['CCAvg'].values.reshape(-1,1))
sns.distplot(ccavg)
plt.show()

"""#### Here we use two types of transforms because PowerTransform is designed specifically to reduce the skewness of the given data but Log Transforms are also good in reducing the skewness. Hence we used both the transforms for transformation."""

# for Mortgage we use binning

data["mortgage"] = pd.cut(data["Mortgage"], bins=[0,100,200,300,400,500,600,700],labels=[0,1,2,3,4,5,6,], include_lowest =True)
data.drop("Mortgage", axis = 1, inplace = True)

data['CCAvg'] = ccavg
data['Income']= income
data.head()



"""## **Data Split 70:30 Ratio :**"""

data.head()

sdata=data[['Age','CCAvg','Income']]
sdata.head()

# Scaling is done to this dataset i.e, standardization is done to the dataset 
from pandas import Series,DataFrame
from sklearn.preprocessing import StandardScaler
Scaler = StandardScaler()
sdata = Scaler.fit_transform(sdata)
print(Scaler.fit_transform(sdata))

sdata = pd.DataFrame(sdata,columns=['Age','Income','CCAvg'])
sdata

data[['Age','Income','CCAvg']] = sdata
data

# Here data is splitted into x and y variables :

x=data.drop(labels = 'Personal Loan', axis=1)
y=data[['Personal Loan']]

# We use ".values" because when we want a value to be a two dimensional array not a dataframe

from sklearn.model_selection import train_test_split
feature_names = x
target_names = y
print("Feature_names:", feature_names)
print("Target_names:",target_names)
x[:10],y

"""#### Splitting Data using Stratified Sampling :"""

# Here data is splitted into x and y variables :

x=data.drop(labels = 'Personal Loan', axis=1)
y=data[['Personal Loan']]

## Now we use "stratified sampling" to the given dataset to satisfy our requirement

xtr,xts,ytr,yts = train_test_split(x,y,test_size = 0.3, stratify=y, random_state = 0)
print(xtr.shape,xts.shape,ytr.shape,yts.shape)

xtr.shape

ytr.shape

ytr

ytr.describe()

"""### ***Types of Classification Algorithms***

## (1) **Logistic Regression**
"""

# Now we build the model and test for tha accuracy

from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
model=LogisticRegression(random_state=0)

ytrn=ytr.to_numpy()
ytrn=ytrn.reshape((3500,))

model = LogisticRegression(random_state=0).fit(xtr,ytrn)

# Training set accuracy for Logistic Regression

model.score(xtr,ytr)

# Testing set accuracy for Logistic Regression

model.score(xts,yts)

# "score()"

model.score(x,y)

# We are predicting the "y" value with the "x" by changing those parameters randomly. So by using those parameters, "y" value is being predicted and by that we are getting the probability of getting loan by the varying parameters of a man.
# now we check the probability with random input i.e x parameters of getting loan

model.predict([[25, 6.827583	,4,	0.845160,	1,	1,	0,	0,	0,	0	]])

"""#### Here we achieve accuracy of 94 percent in both test and train set.

#### *Evaluating the model performace*
"""

import pandas as pd

from pandas import Series,DataFrame

ytrp = model.predict(xtr)

ytrp = pd.DataFrame(ytrp, columns= ['Personal Loan'])

ytrp

ytrp.describe()

ytsp = pd.DataFrame(model.predict(xts), columns= ['Personal Loan'])

ytsp

plt.scatter(yts,ytsp)

"""## *Metrics*

#### we are ready with prediction sets to evaluate metrics
"""

from sklearn import metrics

"""### *Accuracy*"""

metrics.accuracy_score(ytr,ytrp)

metrics.accuracy_score(yts,ytsp)

"""### *Confusion Matrix*"""

metrics.confusion_matrix(ytr,ytrp)

metrics.confusion_matrix(yts,ytsp)

"""### *Recall Score*"""

metrics.recall_score(ytr,ytrp)

metrics.recall_score(yts,ytsp)

"""### *Classification Report*"""

from sklearn.metrics import classification_report

print(classification_report(yts,ytsp))

print(classification_report(ytr,ytrp))

from sklearn import metrics
print('Mean Absolute Error: ', metrics.mean_absolute_error(yts,ytsp))
print('Root Mean Squared Error', np.sqrt(metrics.mean_squared_error(yts,ytsp)))
print('R2 Score', metrics.r2_score(yts,ytsp))

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot
from sklearn import svm

"""### (2) **SVM**"""

# Support vector machine is a representation of the training data as points in space separated into categories by a clear gap that is as wide as possible
#Effective in high dimensional spaces and uses a subset of training points in the decision function so it is also memory efficient

smodel = svm.SVC(random_state=1).fit(xtr,ytrn)

# Training set accuracy for SVM

smodel.score(xtr, ytr)

# Testing set accuracy for SVM

smodel.score(xts, yts)

"""### (3) **Decision Tree**"""

# A Decision Tree produces a sequence of rules that can be used to classify the data
# Decision Tree is simple to understand and visualise, requires little data preparation, and can handle both numerical and categorical data

from sklearn import tree

tmodel = tree.DecisionTreeClassifier(random_state=0).fit(xtr, ytrn)

# Training set accuracy for Decision Tree

tmodel.score(xtr,ytr)

# Testing set accuracy for Decision Tree

tmodel.score(xts,yts)

"""### (4) **Random forest**"""

# Random forest classifier is a meta-estimator that fits a number of decision trees on various sub-samples of datasets
# Reduction in over-fitting and random forest classifier is more accurate than decision trees in most cases

from sklearn.ensemble import RandomForestClassifier

rmodel = RandomForestClassifier(n_estimators=10, random_state=0).fit(xtr,ytrn)

# Training set accuracy for Random Forest

rmodel.score(xtr, ytr)

# Testing set accuracy for Random Forest

rmodel.score(xts, yts)

"""### (5) **K-Nearest Neighbours**"""

# Neighbours based classification is a type of lazy learning 
# This algorithm is simple to implement, robust to noisy training data, and effective if training data is large

from sklearn.neighbors import KNeighborsClassifier

kmodel = KNeighborsClassifier(n_neighbors=15)

kmodel.fit(xtr, ytr)

# Training set accuracy for KNN

kmodel.score(xtr,ytr)

# Testing set accuracy for Logistic Regression

kmodel.score(xts, yts)

"""### (6) **Stochastic Gradient Descent**"""

# Stochastic gradient descent is a simple and very efficient approach to fit linear models
# Efficiency and ease of implementation.

from sklearn.linear_model import SGDClassifier

gmodel = SGDClassifier(loss="modified_huber", shuffle = True, random_state=1)

gmodel.fit(xtr, ytr)

# Training set accuracy for Stochastic Gradient Descent

gmodel.score(xtr, ytr)

# Testing set accuracy for Stochastic Gradient Descent

gmodel.score(xts, yts)

"""### (7) **Naive Bayes**"""

#Naive Bayes algorithm based on Bayes’ theorem
#This algorithm requires a small amount of training data to estimate the necessary parameters

from sklearn.naive_bayes import GaussianNB

nmodel = GaussianNB()

nmodel.fit(xtr, ytr)

# Training set accuracy for  Naive Bayes

nmodel.score(xtr, ytr)

# Testing set accuracy for  Naive Bayes

nmodel.score(xts, yts)

"""### **Conclusion and Business Understanding :**"""

# We import the following libraries for this data analysis

"""#### import numpy as np
#### import pandas as pd
#### import matplotlib.pyplot as plt
#### import seaborn as sns
#### from sklearn.model_selection import train_test_split
#### from sklearn.preprocessing import StandardScaler
#### from sklearn.preprocessing import PowerTransformer
#### from sklearn.linear_model import LogisticRegression
#### from sklearn.tree import DecisionTreeClassifier
#### from sklearn.model_selection import RepeatedStratifiedKFold
#### from sklearn.metrics import accuracy_score

#### *We made the simple step-by-step analysis of customer's characteristics to identify patterns to effectively choose the subset of customers who have a higher probability to buy new product "Personal Loan" from the bank and we check all twelve characteristics whether or not each of them has an association with the product been sold.We build a simple algorithm to make a subset of data.*

#### Here Seven classification algorithms were used in this. From the above graph ,both Decision Tree and Random Forest Classifiers perform very well with our model. We choose Random Forest Classifier since it has slightly better accuracy than Decision Tree. **Random Forest** algorithm have the **highest accuracy** and we can choose that as our final model.
#### **Gradient Boosting classifer** is stable in predicting even scaled or non-scaled data. It can reduce Computation cost. Random Forest predicted the personal loan acceptance more accurately than any other classifier model.
"""

# Observation and Conclusion : "Random Forest" is our Final model